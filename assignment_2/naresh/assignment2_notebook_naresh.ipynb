{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q numpy\n",
    "%pip install -q pandas\n",
    "%pip install -q Pillow\n",
    "%pip install -q tensorflow\n",
    "%pip install -q keras\n",
    "%pip install -q keras-tuner\n",
    "%pip install -q keras.utils\n",
    "%pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        img = img.resize((28, 28))  # Ensure the image is 28x28 pixels\n",
    "        img_array = np.array(img) / 255.0  # Normalize to range [0, 1]\n",
    "        label = os.path.basename(os.path.dirname(img_path))  # Get the folder name as label\n",
    "        return img_array, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {img_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_paths = []\n",
    "    \n",
    "    # Collect all image paths\n",
    "    for label in os.listdir(folder):\n",
    "        label_path = os.path.join(folder, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, filename)\n",
    "                if os.path.isfile(img_path) and img_path.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_paths.append((img_path, label))\n",
    "    \n",
    "    # Use concurrent processing to load images\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(lambda p: process_image(p[0]), img_paths)\n",
    "    \n",
    "    for (img_array, _), (_, label) in zip(results, img_paths):\n",
    "        if img_array is not None:\n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the folders\n",
    "train_folder = 'dataset2/train'\n",
    "test_folder = 'dataset2/test'\n",
    "val_folder = 'dataset2/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and labels\n",
    "x_train, y_train = load_images_from_folder(train_folder)\n",
    "x_test, y_test = load_images_from_folder(test_folder)\n",
    "x_val, y_val = load_images_from_folder(val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all labels to find all unique labels\n",
    "all_labels = np.concatenate([y_train, y_test, y_val])\n",
    "unique_labels = np.unique(all_labels)\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "print(f\"Label to index mapping: {label_to_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all labels to integers using the combined unique labels\n",
    "y_train = np.array([label_to_index[label] for label in y_train])\n",
    "y_test = np.array([label_to_index[label] for label in y_test])\n",
    "y_val = np.array([label_to_index[label] for label in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no out-of-bound indices\n",
    "print(f\"Unique train labels (converted): {np.unique(y_train)}\")\n",
    "print(f\"Unique test labels (converted): {np.unique(y_test)}\")\n",
    "print(f\"Unique validation labels (converted): {np.unique(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to include channel dimension (required by Conv2D)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_val = x_val.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding)\n",
    "num_classes = len(unique_labels)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify one-hot encoding\n",
    "print(f\"Shape of y_train after one-hot encoding: {y_train.shape}\")\n",
    "print(f\"Shape of y_test after one-hot encoding: {y_test.shape}\")\n",
    "print(f\"Shape of y_val after one-hot encoding: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building function\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 3)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperModel class\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Tuning\n",
    "tuner = RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='my_project'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_dataset\u001b[49m,\n\u001b[0;32m      3\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
